{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HYperBert",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3QUASp5SZJFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba1bf58-aaab-4c70-ff06-07559f6e0fe3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 26.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.10.0.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.21.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.13.3)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.24.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.44.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 27.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 66.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIFwSYH_0KDq",
        "outputId": "18c7173e-78e0-4d40-bf38-008922bc5bfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/Development/\n",
            "  inflating: data/Development/dev_definitions.txt  \n",
            "  inflating: data/Development/dev_examples.txt  \n",
            "  inflating: data/Development/dev_hypernyms.txt  \n",
            "  inflating: data/Development/dev_labels.txt  \n",
            "  inflating: data/README.txt         \n",
            "   creating: data/Test/\n",
            "  inflating: data/Test/test_definitions.txt  \n",
            "  inflating: data/Test/test_examples.txt  \n",
            "  inflating: data/Test/test_hypernyms.txt  \n",
            "   creating: data/Training/\n",
            "  inflating: data/Training/train_definitions.txt  \n",
            "  inflating: data/Training/train_examples.txt  \n",
            "  inflating: data/Training/train_hypernyms.txt  \n",
            "  inflating: data/Training/train_labels.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZVe2WcmSCGxy"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import errno\n",
        "import pandas as pd\n",
        "\n",
        "!rm -rf processed_data\n",
        "\n",
        "try:\n",
        "    os.makedirs(\"processed_data\")\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise\n",
        "\n",
        "def decontracted(phrase):\n",
        "\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can't\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"gonna\", \"going to\", phrase)\n",
        "    phrase = re.sub(r\"wanna\", \"want to\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"\\.+\", \" \", phrase)\n",
        "    phrase = re.sub(r\"[^A-Za-z$]\", \" \", phrase)\n",
        "    phrase = re.sub(r\" +\", \" \", phrase)\n",
        "\n",
        "    return phrase\n",
        "\n",
        "\n",
        "def get_df(pref, testing=False):\n",
        "\n",
        "    dataset = []\n",
        "    with open(pref + \"_\" + \"examples.txt\") as examples:\n",
        "        for example in examples:\n",
        "            example = example[:-1].lower().split(\"\\t\")\n",
        "            if example[0] != decontracted(example[0]):\n",
        "                text = re.sub(example[0], \"\\$\", example[2])\n",
        "                text = decontracted(text)\n",
        "                text = re.sub(r\"\\$\", example[0], text)\n",
        "                example[2] = text\n",
        "            else:\n",
        "                example[2] = decontracted(example[2])\n",
        "            dataset.append(example)\n",
        "\n",
        "    with open(pref + \"_\" + \"hypernyms.txt\") as hypernyms:\n",
        "        for index, line in enumerate(hypernyms):\n",
        "            line = line[:-1].lower().split(\"\\t\")\n",
        "            line = \" ; \".join(line)\n",
        "            line = re.sub(r'_', ' ', line)\n",
        "            dataset[index].append(line)\n",
        "\n",
        "    with open(pref + \"_\" + \"definitions.txt\") as definitions:\n",
        "        for index, line in enumerate(definitions):\n",
        "            line = line[:-1].lower().split(\";\")\n",
        "            for i in range(len(line)):\n",
        "                line[i] = line[i].strip()\n",
        "                if dataset[index][0] != decontracted(dataset[index][0]):\n",
        "                    text = re.sub(dataset[index][0], \"\\$\", line[i])\n",
        "                    text = decontracted(text)\n",
        "                    text = re.sub(r\"\\$\", dataset[index][0], text)\n",
        "                    line[i] = text\n",
        "                else:\n",
        "                    line[i] = decontracted(line[i])\n",
        "            line = \" ; \".join(line)\n",
        "            dataset[index].append(line)\n",
        "\n",
        "    cols = [\"target\", \"position\", \"sentence\", \"hypernym\", \"definition\"]\n",
        "\n",
        "    if not testing:\n",
        "        cols.append(\"label\")\n",
        "        with open(pref + \"_\" + \"labels.txt\") as labels:\n",
        "            for index, line in enumerate(labels):\n",
        "                line = line[:-1]\n",
        "                dataset[index].append(line)\n",
        "\n",
        "    df = pd.DataFrame(dataset, columns=cols)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "dev_df = get_df(\"data/Development/dev\")\n",
        "train_df = get_df(\"data/Training/train\")\n",
        "test_df = get_df(\"data/Test/test\", True)\n",
        "\n",
        "dev_df.to_csv(\"processed_data/dev.csv\", index=None)\n",
        "train_df.to_csv(\"processed_data/train.csv\", index=None)\n",
        "test_df.to_csv(\"processed_data/test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "osPsHkWvIDE8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\")"
      ],
      "metadata": {
        "id": "_GWa6yxpYcuU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "def extract_data(filename):\n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    X = {'input_type_ids':[],'input_word_ids':[],'input_mask':[], 'mask1': [], 'mask2': []}\n",
        "    Y = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        word = row['target']\n",
        "        sentence = row['sentence']\n",
        "        hypernyms = row['hypernym']\n",
        "        definitions = row['definition']\n",
        "        if row['label'] == 'T':\n",
        "            label = 1.0\n",
        "        else:\n",
        "            label = 0.0\n",
        "\n",
        "        desc = [definitions]\n",
        "        if isinstance(hypernyms, str):\n",
        "          desc.append(hypernyms)\n",
        "\n",
        "        desc = ' ; '.join(desc)\n",
        "\n",
        "        x = tokenizer(sentence, desc, max_length = 128, padding='max_length', truncation = True)\n",
        "\n",
        "        sep1 = x['input_ids'].index(102)\n",
        "        sep2 = x['input_ids'].index(102, sep1 + 1)\n",
        "\n",
        "        x['mask1'] = np.zeros((128))\n",
        "        x['mask2'] = np.zeros((128))\n",
        "\n",
        "        for i in range(0, sep1):\n",
        "            x['mask1'][i] = 1.0\n",
        "        \n",
        "        for i in range(sep1, sep2):\n",
        "            x['mask2'][i] = 1.0\n",
        "\n",
        "        X['input_type_ids'].append(x['token_type_ids'])\n",
        "        X['input_word_ids'].append(x['input_ids'])\n",
        "        X['input_mask'].append(x['attention_mask'])\n",
        "        X['mask1'].append(x['mask1'])\n",
        "        X['mask2'].append(x['mask2'])\n",
        "\n",
        "        Y.append(label)     \n",
        "\n",
        "    X['input_type_ids'] = np.array(X['input_type_ids'], dtype=np.float32)\n",
        "    X['input_word_ids'] = np.array(X['input_word_ids'], dtype=np.float32)\n",
        "    X['input_mask'] = np.array(X['input_mask'], dtype=np.float32)\n",
        "\n",
        "    X['mask1'] = np.array(X['mask1'], dtype=np.float32)\n",
        "    X['mask2'] = np.array(X['mask2'], dtype=np.float32)\n",
        "\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "X,Y = extract_data('processed_data/train.csv')\n",
        "Xval, Yval = extract_data('processed_data/dev.csv')"
      ],
      "metadata": {
        "id": "mjKWllvTIFvZ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_type_ids = tf.keras.layers.Input(shape=(128,),dtype=tf.int32, name=\"input_type_ids\")\n",
        "input_word_ids = tf.keras.layers.Input(shape=(128,),dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(128,),dtype=tf.int32, name=\"input_mask\")\n",
        "\n",
        "input_mask1 = tf.keras.layers.Input(shape=(128),dtype=tf.float32, name=\"input_mask1\")\n",
        "input_mask2 = tf.keras.layers.Input(shape=(128),dtype=tf.float32, name=\"input_mask2\")\n",
        "\n",
        "pooled_output, sequence_output = bert_encoder([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "def Custom(tensors):\n",
        "    \n",
        "    mask1 = tensors[1]\n",
        "    mask2 = tensors[2]\n",
        "    tensor = tensors[0]\n",
        "\n",
        "    mod_tensor1 = tensor * tf.expand_dims(mask1, axis = 2)\n",
        "    mod_tensor1 = K.sum(mod_tensor1, axis = 1)\n",
        "    mod_tensor1 = mod_tensor1 / tf.expand_dims(K.sum(mask1, axis = 1), axis = 1)\n",
        "\n",
        "    mod_tensor2 = tensor * tf.expand_dims(mask2, axis = 2)\n",
        "    mod_tensor2 = K.sum(mod_tensor2, axis = 1)\n",
        "    mod_tensor2 = mod_tensor2 / tf.expand_dims(K.sum(mask2, axis = 1), axis = 1)\n",
        "\n",
        "    return mod_tensor1, mod_tensor2\n",
        "\n",
        "seq1, seq2 = tf.keras.layers.Lambda(Custom)([sequence_output, input_mask1, input_mask2])\n",
        "\n",
        "x = tf.keras.layers.Concatenate()([seq1, seq2, pooled_output])\n",
        "x = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "model = tf.keras.models.Model(\n",
        "      inputs=[\n",
        "        input_word_ids,\n",
        "        input_mask,\n",
        "        input_type_ids,\n",
        "        input_mask1,\n",
        "        input_mask2], \n",
        "      outputs=x)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmJ4l6L2YIz7",
        "outputId": "79ef6622-cb8b-47f8-fabb-9ffc6c01fd4b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)        [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       [(None, 768),        109482241   ['input_word_ids[0][0]',         \n",
            "                                 (None, 128, 768)]                'input_mask[0][0]',             \n",
            "                                                                  'input_type_ids[0][0]']         \n",
            "                                                                                                  \n",
            " input_mask1 (InputLayer)       [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_mask2 (InputLayer)       [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " lambda_54 (Lambda)             ((None, 768),        0           ['keras_layer[56][1]',           \n",
            "                                 (None, 768))                     'input_mask1[0][0]',            \n",
            "                                                                  'input_mask2[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_26 (Concatenate)   (None, 2304)         0           ['lambda_54[0][0]',              \n",
            "                                                                  'lambda_54[0][1]',              \n",
            "                                                                  'keras_layer[56][0]']           \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 1)            2305        ['concatenate_26[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,484,546\n",
            "Trainable params: 2,305\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([X['input_word_ids'], X['input_mask'], X['input_type_ids'], X['mask1'], X['mask2']], Y, \n",
        "          epochs = 20, batch_size = 16,\n",
        "          validation_data = ([Xval['input_word_ids'], Xval['input_mask'], Xval['input_type_ids'], Xval['mask1'], Xval['mask2']], Yval)\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySbknbfzYmVf",
        "outputId": "be667913-a30f-41f2-be2f-13b9bfa845f0"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "133/133 [==============================] - 26s 181ms/step - loss: 0.6303 - accuracy: 0.6555 - val_loss: 0.6568 - val_accuracy: 0.6328\n",
            "Epoch 2/20\n",
            "133/133 [==============================] - 25s 190ms/step - loss: 0.5696 - accuracy: 0.7072 - val_loss: 0.6321 - val_accuracy: 0.6432\n",
            "Epoch 3/20\n",
            "133/133 [==============================] - 23s 177ms/step - loss: 0.5589 - accuracy: 0.7185 - val_loss: 0.6846 - val_accuracy: 0.6693\n",
            "Epoch 4/20\n",
            "133/133 [==============================] - 25s 189ms/step - loss: 0.5365 - accuracy: 0.7307 - val_loss: 0.6398 - val_accuracy: 0.6771\n",
            "Epoch 5/20\n",
            "133/133 [==============================] - 25s 189ms/step - loss: 0.5353 - accuracy: 0.7387 - val_loss: 0.6067 - val_accuracy: 0.6875\n",
            "Epoch 6/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.5118 - accuracy: 0.7486 - val_loss: 0.6800 - val_accuracy: 0.6823\n",
            "Epoch 7/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.4974 - accuracy: 0.7632 - val_loss: 0.6042 - val_accuracy: 0.6641\n",
            "Epoch 8/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.4886 - accuracy: 0.7664 - val_loss: 0.6078 - val_accuracy: 0.6771\n",
            "Epoch 9/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.4920 - accuracy: 0.7650 - val_loss: 0.6065 - val_accuracy: 0.7031\n",
            "Epoch 10/20\n",
            "133/133 [==============================] - 25s 189ms/step - loss: 0.4793 - accuracy: 0.7763 - val_loss: 0.6462 - val_accuracy: 0.6615\n",
            "Epoch 11/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.4639 - accuracy: 0.7857 - val_loss: 0.6258 - val_accuracy: 0.6901\n",
            "Epoch 12/20\n",
            "133/133 [==============================] - 24s 178ms/step - loss: 0.4582 - accuracy: 0.7961 - val_loss: 0.6049 - val_accuracy: 0.6667\n",
            "Epoch 13/20\n",
            "133/133 [==============================] - 25s 189ms/step - loss: 0.4560 - accuracy: 0.7899 - val_loss: 0.6189 - val_accuracy: 0.7031\n",
            "Epoch 14/20\n",
            "133/133 [==============================] - 23s 177ms/step - loss: 0.4463 - accuracy: 0.8026 - val_loss: 0.5972 - val_accuracy: 0.6953\n",
            "Epoch 15/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.4418 - accuracy: 0.7993 - val_loss: 0.6053 - val_accuracy: 0.6927\n",
            "Epoch 16/20\n",
            "133/133 [==============================] - 25s 189ms/step - loss: 0.4334 - accuracy: 0.8102 - val_loss: 0.6076 - val_accuracy: 0.6693\n",
            "Epoch 17/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.4284 - accuracy: 0.8172 - val_loss: 0.6288 - val_accuracy: 0.7161\n",
            "Epoch 18/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.4285 - accuracy: 0.8097 - val_loss: 0.6105 - val_accuracy: 0.6771\n",
            "Epoch 19/20\n",
            "133/133 [==============================] - 25s 189ms/step - loss: 0.4195 - accuracy: 0.8167 - val_loss: 0.6637 - val_accuracy: 0.6797\n",
            "Epoch 20/20\n",
            "133/133 [==============================] - 24s 177ms/step - loss: 0.4231 - accuracy: 0.8144 - val_loss: 0.6046 - val_accuracy: 0.6901\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f75dbc52dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LAnuAChfQ1cn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}